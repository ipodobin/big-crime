{
  "metadata": {
    "name": "streaming",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.pyflink\n\nfrom pyflink.table import EnvironmentSettings, TableEnvironment, DataTypes\nfrom pyflink.table.window import Tumble\nfrom pyflink.table.udf import udf\nimport os\nimport json\n\n# 1. Creates a Table Environment\nenv_settings \u003d EnvironmentSettings.in_streaming_mode()\ntable_env \u003d TableEnvironment.create(env_settings)\n\n\ndef create_input_table(table_name, stream_name, region, stream_initpos):\n    return \"\"\" CREATE TABLE {0} (\n                `id` BIGINT,\n                `date` TIMESTAMP(3),\n                `block` STRING,\n                `iucr` STRING,\n                `primary_type` STRING,\n                `description` STRING,\n                `location_description` STRING,\n                `arrest` STRING,\n                `domestic` STRING,\n                `beat` STRING,\n                `district` STRING,\n                `ward` STRING,\n                `community_area` STRING,\n                WATERMARK FOR  `date` AS  `date` - INTERVAL \u00275\u0027 SECOND\n              )\n              PARTITIONED BY (community_area)\n              WITH (\n                \u0027connector\u0027 \u003d \u0027kinesis\u0027,\n                \u0027stream\u0027 \u003d \u0027{1}\u0027,\n                \u0027aws.region\u0027 \u003d \u0027{2}\u0027,\n                \u0027scan.stream.initpos\u0027 \u003d \u0027{3}\u0027,\n                \u0027format\u0027 \u003d \u0027json\u0027\n              ) \"\"\".format(table_name, stream_name, region, stream_initpos)\n\ndef create_output_table(table_name, stream_name, region):\n    return \"\"\" CREATE TABLE {0} (\n                community_area STRING,\n                arrest STRING,\n                cnt BIGINT,\n                event_time VARCHAR(64)\n              )\n              PARTITIONED BY (community_area)\n              WITH (\n                \u0027connector\u0027 \u003d \u0027kinesis\u0027,\n                \u0027stream\u0027 \u003d \u0027{1}\u0027,\n                \u0027aws.region\u0027 \u003d \u0027{2}\u0027,\n                \u0027format\u0027 \u003d \u0027json\u0027\n              ) \"\"\".format(table_name, stream_name, region)\n\n\ndef perform_tumbling_window_aggregation(input_table_name):\n    # use SQL Table in the Table API\n    input_table \u003d table_env.from_path(input_table_name)\n\n    tumbling_window_table \u003d (\n        input_table.window(\n            Tumble.over(\"10.seconds\").on(\"date\").alias(\"ten_second_window\")\n        )\n        .group_by(\"community_area, arrest, ten_second_window\")\n        .select(\"community_area, arrest, count(1) as cnt, to_string(ten_second_window.end) as event_time\")\n    )\n\n    return tumbling_window_table\n\n\n@udf(input_types\u003d[DataTypes.TIMESTAMP(3)], result_type\u003dDataTypes.STRING())\ndef to_string(i):\n    return str(i)\n\n\ntable_env.create_temporary_system_function(\"to_string\", to_string)\n\ndef main():\n    # tables\n    input_table_name \u003d \"big_crime_input_table\"\n    output_table_name \u003d \"output_table\"\n\n    input_stream \u003d \u0027big-crime-stream-crimes\u0027\n    input_region \u003d \u0027us-east-1\u0027\n    stream_initpos \u003d \u0027LATEST\u0027\n\n    output_stream \u003d \u0027big-crime-stream-summary\u0027\n    output_region \u003d\u0027us-east-1\u0027\n\n    # 2. Creates a source table from a Kinesis Data Stream\n    table_env.execute_sql(create_input_table(input_table_name, input_stream, input_region, stream_initpos))\n\n    # 3. Creates a sink table writing to a Kinesis Data Stream\n    table_env.execute_sql(create_output_table(output_table_name, output_stream, output_region))\n\n    # 4. Queries from the Source Table and creates a tumbling window over 10 seconds to calculate the count of events\n    # over the window.\n    tumbling_window_table \u003d perform_tumbling_window_aggregation(input_table_name)\n    table_env.create_temporary_view(\"tumbling_window_table\", tumbling_window_table)\n\n    # 5. These tumbling windows are inserted into the sink table\n    table_result \u003d table_env.execute_sql(\"INSERT INTO {0} SELECT * FROM {1}\"\n                                         .format(output_table_name, \"tumbling_window_table\"))\n\n    print(table_result.get_job_client().get_job_status())\n\n\nif __name__ \u003d\u003d \"__main__\":\n    main()"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.ssql(type\u003dupdate)\nselect * from output_table"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.ssql\n"
    }
  ]
}