{
  "metadata": {
    "name": "streaming",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.pyflink\n# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n# SPDX-License-Identifier: MIT-0\n# -*- coding: utf-8 -*-\n\n\"\"\"\nsliding-windows.py\n~~~~~~~~~~~~~~~~~~~\nThis module:\n    1. Creates a table environment\n    2. Creates a source table from a Kinesis Data Stream\n    3. Creates a sink table writing to a Kinesis Data Stream\n    4. Queries from the Source Table and\n       creates a sliding window over 10 seconds to calculate the minimum value over the window.\n    5. These sliding window results are inserted into the Sink table.\n\"\"\"\n\nfrom pyflink.table import EnvironmentSettings, TableEnvironment, DataTypes\nfrom pyflink.table.window import Slide\nfrom pyflink.table.udf import udf\nimport os\nimport json\n\n# 1. Creates a Table Environment\nenv_settings \u003d EnvironmentSettings.in_streaming_mode()\ntable_env \u003d TableEnvironment.create(env_settings)\n# env \u003d StreamExecutionEnvironment.get_execution_environment()\n# table_env \u003d StreamTableEnvironment.create(env)\n\n\ndef create_input_table(table_name, stream_name, region, stream_initpos):\n    return \"\"\" CREATE TABLE {0} (\n                event_time TIMESTAMP(3),\n                case_number STRING,\n                block STRING,\n                primary_type STRING,\n                location_description STRING,\n                arrest STRING,\n                district STRING,\n                ward STRING,\n                community_area STRING,\n                WATERMARK FOR event_time AS event_time - INTERVAL \u00275\u0027 SECOND\n              )\n              PARTITIONED BY (community_area)\n              WITH (\n                \u0027connector\u0027 \u003d \u0027kinesis\u0027,\n                \u0027stream\u0027 \u003d \u0027{1}\u0027,\n                \u0027aws.region\u0027 \u003d \u0027{2}\u0027,\n                \u0027scan.stream.initpos\u0027 \u003d \u0027{3}\u0027,\n                \u0027format\u0027 \u003d \u0027json\u0027,\n                \u0027json.timestamp-format.standard\u0027 \u003d \u0027ISO-8601\u0027\n              ) \"\"\".format(table_name, stream_name, region, stream_initpos)\n\ndef create_output_table(table_name, stream_name, region):\n    return \"\"\" CREATE TABLE {0} (\n                community_area STRING,\n                case_count BIGINT,\n                event_time VARCHAR(64)\n              )\n              PARTITIONED BY (community_area)\n              WITH (\n                \u0027connector\u0027 \u003d \u0027kinesis\u0027,\n                \u0027stream\u0027 \u003d \u0027{1}\u0027,\n                \u0027aws.region\u0027 \u003d \u0027{2}\u0027,\n                \u0027format\u0027 \u003d \u0027json\u0027,\n                \u0027json.timestamp-format.standard\u0027 \u003d \u0027ISO-8601\u0027\n              ) \"\"\".format(table_name, stream_name, region)\n\n\ndef perform_sliding_window_aggregation(input_table_name):\n    # use SQL Table in the Table API\n    input_table \u003d table_env.from_path(input_table_name)\n\n    sliding_window_table \u003d (\n        input_table\n            .window(\n                Slide.over(\"1.minutes\")\n                .every(\"30.seconds\")\n                .on(\"event_time\")\n                .alias(\"one_minute_window\")\n            )\n            .group_by(\"community_area, one_minute_window\")\n            .select(\"community_area, case_number.count as case_count, to_string(one_minute_window.end) as event_time\")\n            .where(\"case_count \u003e 1000\")\n    )\n\n    return sliding_window_table\n\n\n@udf(input_types\u003d[DataTypes.TIMESTAMP(3)], result_type\u003dDataTypes.STRING())\ndef to_string(i):\n    return str(i)\n\n\ntable_env.create_temporary_system_function(\"to_string\", to_string)\n\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.pyflink\n\n# tables\ninput_table_name \u003d \"input_table\"\noutput_table_name \u003d \"output_table\"\n\n# 2. Creates a source table from a Kinesis Data Stream\ntable_env.execute_sql(create_input_table(input_table_name, \"big-crime-stream-crimes\", \"us-east-1\", \"LATEST\"))\n\n# 3. Creates a sink table writing to a Kinesis Data Stream\ntable_env.execute_sql(create_output_table(output_table_name, \"big-crime-stream-summary\", \"us-east-1\"))\n"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.pyflink\n\n# 4. Queries from the Source Table and creates a sliding window over 10 seconds to calculate the minimum value\n# over the window.\nsliding_window_table \u003d perform_sliding_window_aggregation(input_table_name)\ntable_env.create_temporary_view(\"sliding_window_table\", sliding_window_table)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.pyflink\n\n# 5. These sliding windows are inserted into the sink table\ntable_result1 \u003d table_env.execute_sql(\"INSERT INTO {0} SELECT community_area, case_count, event_time FROM {1}\"\n                                          .format(output_table_name, \"sliding_window_table\"))\n\n\njob_status \u003d table_result1.get_job_client().get_job_status()"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.pyflink\n\njob_status.cancel()\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.ssql\n"
    }
  ]
}